# Implementation Plan

Phases 1-5 complete. All 93 tests pass (88 oauth-proxy + 5 common). Binary sizes well under 15MB target. Specs updated with resolved decisions.

Audits 1-33: Found and fixed 55+ issues across 33 audits including 5 bugs, spec documentation gaps, K8s security context issues, state machine correctness, metrics configuration, RUNBOOK accuracy, and dependency upgrades (reqwest 0.12→0.13, toml 0.8→0.9, metrics-exporter-prometheus 0.16→0.18). Recent audits (29-33) found increasingly fewer issues as the codebase stabilized: startup probe gap (32nd), RUNBOOK inaccuracies (33rd), and clean results (29th, 30th, 31st).

Thirty-fourth audit (v0.0.48): Comprehensive Opus-level audit across all 10 dimensions: 10 Rust source files, 2 specs, 7 K8s manifests, Dockerfile, CI workflow, RUNBOOK, and example config. 0 issues found. Cross-referenced every type, state transition, metric name/label, error type value, retry parameter, health endpoint field, config schema entry, PromQL query, K8s security context, and environment variable between specs, RUNBOOK, and implementation — all consistent. Lockfile at latest compatible versions (matchit 0.8.4 constrained by axum). All 80 tests pass, clippy clean, formatting clean.

Thirty-fifth audit (v0.0.51): Fixed two CI/CD blockers preventing Docker image push to GHCR. (1) Added `docker/setup-buildx-action@v3` — the `cache-from/cache-to: type=gha` options require a buildx docker-container driver, not the default docker driver. (2) Renamed Dockerfile user from `proxy` to `appuser` — `debian:bookworm-slim` now ships with a system user named `proxy` (UID 13), causing `useradd` to fail with "user already exists". Both fixes committed and pushed. CI confirmed successful.

Thirty-sixth audit (v0.0.54): Live cluster verification. Confirmed CI Docker builds succeed and images push to GHCR. Discovered the "Make package public" CI step was a silent no-op — `GITHUB_TOKEN` cannot change GHCR package visibility (GitHub API limitation). Removed the broken step. K8s pods in `ErrImagePull`/`ImagePullBackOff` due to private package + inadequate pull secret token (gh CLI `gho_` token lacks `read:packages`). All 83 tests pass, clippy clean, fmt clean. Root cause documented: making the package public via GitHub web UI is a one-time manual prerequisite.

Thirty-seventh audit (v0.0.55): Live cluster debugging. Found tailscaled sidecar crashing with `store.New failed: creating state directory: chmod /var/lib/tailscale: operation not permitted; starting with in-memory store` followed by `500 Internal Server Error: cannot start backend when state store is unhealthy`. Root cause: `capabilities: drop: ["ALL"]` removes `CAP_FOWNER`, which tailscale v1.94.1 needs to `chmod` its state directory. Fix: added `capabilities: add: ["FOWNER"]` to the tailscaled container security context. The proxy container retains the strict `drop: ["ALL"]` with no adds. GHCR 403 remains the primary blocker — package still private.

Thirty-eighth audit (v0.0.56): Comprehensive Opus-level audit of all source files, specs, K8s manifests, CI, RUNBOOK. 0 spec-vs-implementation discrepancies found. Identified 8 test coverage gaps (all edge cases, not behavioral bugs) and 8 code quality observations (all by-design tradeoffs). Added 5 new tests covering the most impactful gaps: state machine edge cases (Draining+RequestReceived/RequestCompleted, Starting+unexpected events, ConnectingTailnet+ListenerReady) and proxy header injection resilience (invalid header names/values skip gracefully without affecting valid injections). Test count: 85 oauth-proxy + 3 common = 88.

GHCR access investigation: `gh auth` token lacks `read:packages`/`write:packages` scopes. `gh auth refresh -s read:packages,write:packages` requires interactive browser authentication. Alternative: make the package public via GitHub web UI (Settings → Packages → Change visibility), or use a classic PAT with `write:packages` scope.

Thirty-ninth audit (v0.0.58): Updated CI workflow — actions/checkout v4→v6, added explicit least-privilege permissions blocks to all CI jobs (CI run confirmed successful). Dependencies audit: all 16 workspace dependencies match spec exactly, Cargo.lock at latest compatible versions. Comprehensive source file review found 0 bugs or spec discrepancies. Added 2 error Display/Debug formatting tests. K8s deployment confirmed stuck in ImagePullBackOff due to GHCR 403 (known blocker). ts-authkey secret missing from cluster — must be created imperatively after GHCR fix. All 90 tests pass, clippy clean, fmt clean.

Fortieth audit (v0.0.59): Opus-level audit across all source files, specs, K8s manifests, RUNBOOK. Found 4 issues: (1) IMPLEMENTATION_PLAN.md referenced `ts-authkey` but K8s manifests and RUNBOOK use `tailscale-authkey` — fixed documentation inconsistency. (2) Retry delay in proxy.rs was a magic number `Duration::from_millis(100)` — extracted to named constants `MAX_UPSTREAM_ATTEMPTS` and `UPSTREAM_RETRY_DELAY` matching service.rs pattern. (3) `common::Error` had no Display/Debug formatting tests — added 2 tests. (4) RUNBOOK used "10MB" instead of "10 MiB" for body size limit — fixed 3 occurrences. All 92 tests pass (87 oauth-proxy + 5 common), clippy clean, fmt clean.

Forty-first audit (v0.0.60): Comprehensive Opus-level audit of all 10 Rust source files, 2 specs, 7 K8s manifests, Dockerfile, CI workflow, RUNBOOK, and example config. Found 1 issue: ConfigMap `state_dir` path was `/var/lib/anthropic-oauth-proxy/tailscale` but the K8s deployment mounts the tailscaled state volume at `/var/lib/tailscale` (matching `TS_STATE_DIR` env var). While `state_dir` is unused by the Rust service (sidecar manages state), the mismatch was misleading for operators. Fixed in ConfigMap, spec example, and example config. All 92 tests pass, clippy clean, fmt clean.

Forty-second audit (v0.0.61): Comprehensive Opus-level audit found 6 issues across code, K8s manifests, CI, and Dockerfile. (1) Body size limit `10 * 1024 * 1024` was a magic number duplicated in proxy.rs and test echo server — extracted to `pub const MAX_BODY_SIZE` in proxy.rs, referenced from tests. (2) reqwest client settings `connect_timeout(5s)` and `pool_max_idle_per_host(100)` were inline magic numbers — extracted to named constants `CONNECT_TIMEOUT` and `POOL_MAX_IDLE_PER_HOST` in main.rs. (3) Dockerfile missing `EXPOSE 8080` — added for documentation and tooling. (4) K8s deployment using mutable `:main` tag without `imagePullPolicy: Always` — added to proxy container so updated images are always pulled. (5) CI workflow `on` trigger had no `tags` pattern, making `type=semver` Docker metadata unreachable — added `tags: ["v*"]` trigger and updated push condition to include tag refs. (6) No Prometheus scrape mechanism for K8s — added `prometheus.io/scrape`, `prometheus.io/port`, and `prometheus.io/path` annotations to pod template. Cluster confirmed accessible (Talos Omni online, kubectl authenticated). Pods still in ImagePullBackOff due to known GHCR 403 blocker. tailscale-authkey secret exists in cluster. All 92 tests pass, clippy clean, fmt clean.

Forty-third audit (v0.0.62): Comprehensive Opus-level audit across all 10 Rust source files, 2 specs, 7 K8s manifests, Dockerfile, CI workflow, RUNBOOK, and example config. 0 bugs, 0 spec violations, 0 security issues found. All metric names, labels, error types, state transitions, config fields, and dependency versions verified consistent across specs, code, RUNBOOK, and K8s manifests. Found 3 improvements: (1) No `.dockerignore` file — Docker build context included `.git/`, `.specstory/`, `k8s/`, and other unnecessary files. Added `.dockerignore` excluding build artifacts, git metadata, K8s manifests, documentation, specs, and CI files. (2) `ghcr-pull-secret` referenced in deployment.yaml but not documented in RUNBOOK — added creation instructions to the Initial Deploy section. (3) Missing edge case test for `TS_AUTHKEY` env var overriding a nonexistent `auth_key_file` path — added test confirming env var precedence skips the file read entirely. All 93 tests pass (88 oauth-proxy + 5 common), clippy clean, fmt clean.

Forty-fourth audit (v0.0.63): Live cluster deployment fix. Tailscaled sidecar was CrashLoopBackOff with `chmod /var/lib/tailscale: operation not permitted`. Root cause: `CAP_FOWNER` added in 37th audit is inert on containerd — containerd clears ambient capabilities for non-root containers, so added capabilities never reach the effective set (containerd issue #5644, closed "not planned"; KEP-2763 ambient capabilities unimplemented). The real trigger is tailscale's `ensureStateDirPermsUnix` function which sees `fsGroup`-applied permissions `2755` (setgid) on the state dir, compares against expected `0700`, and calls `chmod`. The function has a guard: it only runs when `filepath.Base(dir) == "tailscale"`. Fix: renamed state dir from `/var/lib/tailscale` to `/var/lib/ts-state` across deployment, configmap, example config, spec, RUNBOOK, and test fixtures. Also removed the inert `CAP_FOWNER` capability, tightening the security posture. Deployment now fully operational: both containers 2/2 Running, health endpoint returns `{"status":"healthy","tailnet":"connected","tailnet_hostname":"anthropic-oauth-proxy","tailnet_ip":"100.72.121.123"}`, proxy successfully forwards to `api.anthropic.com`. All 93 tests pass, clippy clean, fmt clean.

## Remaining Work

- [x] ~~GHCR package visibility~~ — Package made public (2026-02-06). ImagePullBackOff blocker cleared.
- [x] Create tailscale-authkey K8s secret — exists in cluster (confirmed via `kubectl get secrets -n anthropic-oauth-proxy`)
- [x] Verify deployment — both containers running, health endpoint healthy, proxy forwarding to upstream confirmed
- [x] Test MagicDNS hostname resolution — `curl http://anthropic-oauth-proxy:8080/health` returns healthy response, proxy accessible from tailnet via MagicDNS at `100.72.121.123`
- [ ] Aperture config update — route `http://ai/` to the proxy (requires Aperture configuration)
- [ ] Production monitoring — observe live traffic
- [ ] Verify ACL connectivity from Aperture (requires Aperture configured to route through proxy)

## Known Limitations

- Health endpoint `tailnet` state is set once at startup and never updated during operation. If tailscaled drops during runtime, health still reports `"connected"`. Fixing this requires tailnet health monitoring (periodic polling of tailscaled), which is infrastructure work beyond the current spec. The `tailnet_connected` Prometheus gauge does get set to `false` during graceful shutdown.
- `ConfigError` and `ListenerBindError` are not in the service's Rust error enum. Config errors use `common::Error` and listener bind errors use `anyhow`. These paths work correctly; the spec now documents this split explicitly.

## Learnings

- Reverse proxies must strip the client's `host` header before forwarding. The client sends `Host: <proxy-address>` but the upstream expects `Host: <upstream-address>`. HTTP client libraries like reqwest automatically set the correct Host from the URL, but only if the incoming Host isn't manually set in the header map.
- Config-driven header injection must protect safety-critical headers. The `authorization` header should never be overwritable via config, even if someone misconfigures it. This is a system boundary validation.
- When copying HTTP headers in a proxy, use `append()` not `insert()` to preserve multi-value headers. `insert()` replaces, `append()` accumulates. This matters for headers like Cookie, Accept-Encoding, and custom multi-value headers.
- Rust 2024 edition requires `unsafe {}` blocks inside `unsafe fn` bodies. Tests that call `std::env::set_var`/`remove_var` (unsafe since Rust 1.83) need both the `unsafe fn` wrapper and inner `unsafe {}` blocks.
- Tests that mutate environment variables must be serialized with a `Mutex` to prevent data races when `cargo test` runs in parallel (default behavior). Without this, env-var-dependent tests fail nondeterministically.
- `tracing-subscriber` requires the `env-filter` feature for `EnvFilter` support. The `json` feature alone is not sufficient.
- Drain coordination: axum's `with_graceful_shutdown` handles connection-level draining (stops accepting new connections, waits for in-flight to finish), but it waits indefinitely by default. The spec requires a 5-second drain timeout. Enforced by spawning the server as a task, signaling it via a `oneshot` channel on SIGTERM/SIGINT, then racing the drain against `DRAIN_TIMEOUT` using `tokio::time::timeout`.
- `tailscale-localapi` v0.4.2 uses `chrono` for timestamps and brings in `hyper` v0.14 (in addition to the workspace's `hyper` v1). This is expected — the crate was built against an older `hyper` API.
- On macOS with the App Store Tailscale variant, there is no Unix socket. The fallback is `tailscale status --json` which parses the same `Status` type via `serde_json`.
- `metrics-exporter-prometheus` global recorder can only be installed once per process. In tests, use `PrometheusBuilder::build_recorder()` + `.handle()` to create isolated instances without global installation.
- Integration tests using `tower::ServiceExt::oneshot` give full end-to-end coverage without needing to bind a TCP port — they call the axum router directly as a tower Service.
- Cross-compilation from macOS to Linux requires `cargo-zigbuild` (uses zig as the C cross-linker). Standard `cargo build --target` fails because `aws-lc-sys` needs a C cross-compiler.
- `reqwest` with default features enabled pulls in `native-tls` → `openssl-sys` on Linux targets, even when `rustls-tls` is also enabled. Setting `default-features = false` is required to avoid the OpenSSL dependency.
- `tower` crate requires explicit feature flags for each layer type. `ConcurrencyLimitLayer` requires the `limit` feature.
- `BackendState` enum in `tailscale-localapi` is `#[non_exhaustive]`, requiring wildcard match arms.
- Tower's `ConcurrencyLimitLayer` queues excess requests rather than rejecting them. Requests above `max_connections` will wait (not fail) until a slot opens.
- Docker build uses native `x86_64-unknown-linux-gnu` target (not musl) inside `rust:1-bookworm`. No cross-compilation needed since Docker IS Linux.
- K8s manifests use `TS_USERSPACE=true` for the tailscaled sidecar to avoid requiring `NET_ADMIN` capabilities. The proxy and tailscaled share the Unix socket via an `emptyDir` volume.
- GitHub Actions CI uses `actions/checkout@v6`, `dtolnay/rust-toolchain@stable`, and `Swatinem/rust-cache@v2`. All jobs have explicit least-privilege `permissions` blocks. Docker job uses `docker/build-push-action@v6` with GHA cache. Images push to GHCR using the built-in `GITHUB_TOKEN`.
- `BackendState::NeedsMachineAuth` requires manual admin approval in the Tailscale console. Mapping it to a retryable error wastes 31 seconds of exponential backoff before giving up. It must be non-retryable.
- A spec-vs-implementation audit is valuable after completing major phases. Found 43+ discrepancies across ten audits including 5 bugs, spec documentation gaps, and positive deviations. The tenth audit found 1 state machine bug (terminal state not fully inert).
- Terminal states in a state machine must be explicitly guarded before wildcard match arms. Without a `Stopped` guard before `(_, ShutdownSignal)`, the wildcard produces a `Shutdown` action from an already-stopped state, violating the "terminal means inert" invariant.
- K8s sidecar pattern requires both containers to mount the shared volume. The volume definition in `spec.volumes` is not enough — each container that needs the socket must have a `volumeMount` entry. Easy to miss because the tailscaled container (which creates the socket) works fine; only the consumer (proxy) fails.
- Response bodies must be streamed, not buffered, in a proxy targeting the Claude API. The Anthropic API uses SSE (Server-Sent Events) for streaming responses. Buffering breaks real-time delivery and uses unbounded memory. Use `reqwest::Response::bytes_stream()` with `axum::body::Body::from_stream()`. Metrics (status, duration) must be collected before consuming the stream since headers are available immediately.
- Config validation at system boundaries catches misconfigurations early: `upstream_url` must have an http(s) scheme, `timeout_secs` and `max_connections` must be non-zero. Without URL scheme validation, reqwest fails at request time with a confusing error instead of at startup.
- `metrics-exporter-prometheus` renders `metrics::histogram!()` as a Prometheus summary (quantiles) by default. To get a true histogram (with `_bucket` lines needed by `histogram_quantile()` queries), you must configure explicit bucket boundaries via `set_buckets_for_metric()`. Without this, RUNBOOK PromQL queries referencing `_bucket` will fail silently.
- In a sidecar pattern, secrets should only be mounted in the container that consumes them. `TS_AUTHKEY` belongs on the tailscaled sidecar, not the proxy container — the proxy queries tailnet state via the Unix socket and never authenticates directly.
- Spec dependency lists can drift from the actual Cargo.toml when features are added during implementation. The `"stream"` feature on reqwest was added for response streaming but the spec's Build & Distribution section was not updated. Always update the spec when adding dependency features.
- Dockerfiles for K8s pods with `runAsNonRoot: true` must create the non-root user in the image. `debian:bookworm-slim` only has root; use `useradd -u 1000 -r -s /sbin/nologin appuser` and `USER 1000` in the runtime stage. Without this, the pod crashes with `CreateContainerConfigError`. Avoid the username `proxy` — it's a Debian standard system user (UID 13).
- `reqwest::Client::new()` uses unbounded connection pool defaults. For a proxy with configurable `max_connections`, set `connect_timeout()` and `pool_max_idle_per_host()` on the builder to prevent unbounded TCP connections when upstream is slow to accept.
- K8s Pod Security Standards (restricted profile) require `allowPrivilegeEscalation: false`, `readOnlyRootFilesystem: true`, and `capabilities: { drop: ["ALL"] }` on every container. Missing these can block deployment to hardened clusters.
- Using `:latest` for sidecar images in K8s deployments breaks reproducibility and rollbacks. Pin to specific versions (e.g. `tailscale:v1.94.1`) so that `kubectl rollout undo` works predictably.
- State machine variants should only carry data they own and use. The `Running` state had a `ServiceMetrics` that was never read because `main.rs` creates its own metrics instance wired to `ProxyState`. Dead allocations in state variants waste memory and confuse readers.
- K8s `terminationGracePeriodSeconds` should be DRAIN_TIMEOUT + small buffer (e.g. 1s), not significantly larger. The application force-exits after DRAIN_TIMEOUT regardless, so the extra Kubernetes wait is wasted delay during rolling updates and node drains.
- Kustomize secrets with placeholder values overwrite real secrets on `kubectl apply -k`. If a secret contains a real credential created imperatively, do NOT include it in `kustomization.yaml`. Keep a schema-documenting `secret.yaml` in the repo but excluded from kustomization resources. The RUNBOOK should instruct users to create the secret imperatively after `kubectl apply -k`.
- K8s Pod Security Standards restricted profile requires `runAsNonRoot: true` on every container, not just the main application container. Setting `runAsUser: 1000` is not sufficient — the explicit `runAsNonRoot` field is what Kubernetes admission controllers check. Missing it on sidecar containers is easy to overlook.
- Prometheus histograms and summaries are different metric types with different semantics. Histograms produce `_bucket`, `_sum`, and `_count` lines; quantiles are computed at query time via `histogram_quantile()`. Summaries compute quantiles client-side. Documentation must use precise terminology — saying a histogram "automatically computes quantiles" is misleading and confuses operators writing PromQL.
- Undocumented environment variable overrides create debugging blind spots. If code reads an env var to override defaults (like `TAILSCALE_SOCKET` for the socket path), it must be documented in both the spec's environment variables table and the operational runbook's troubleshooting section.
- Crate `derive` features (e.g. `zeroize = { features = ["derive"] }`) pull in proc-macro dependencies (`syn`, `quote`, `proc-macro2`). Only enable them if `#[derive(Trait)]` is actually used. Using a trait as a bound or calling methods directly does not require the derive feature.
- Concurrency limits on a proxy must exclude observability endpoints. K8s liveness/readiness probes and Prometheus scrapes must always be responsive regardless of proxy load. In axum, use `Router::merge()` to nest a concurrency-limited sub-router (proxy routes) under an unlimited parent router (health/metrics routes).
- K8s secret rotation should use `kubectl create --dry-run=client -o yaml | kubectl apply -f -` for atomic updates. A `delete` then `create` sequence leaves a window where pods rescheduled between the two commands fail with `CreateContainerConfigError`.
- Minimal Docker images (debian-slim + ca-certificates only) don't have debugging tools like `curl`. RUNBOOK troubleshooting steps should use `kubectl port-forward` from the operator's workstation instead of `kubectl exec` with tools that aren't in the image.
- K8s restricted pod security profile requires pod-level `securityContext` with `seccompProfile.type: RuntimeDefault`. Container-level security contexts alone are insufficient — admission controllers check the pod-level seccomp profile separately. Also set `fsGroup` at the pod level so emptyDir volumes are writable by the non-root group.
- `unreachable!()` in non-test code paths is a latent process abort, especially with `panic = "abort"` in the release profile. Even if the current caller never triggers the arm, future code changes might. Replace `unreachable!()` with defensive no-op returns in state machines where the arm is theoretically reachable but practically unused.
- K8s resources should carry consistent `app:` labels even if they are not selected by anything. Labels enable `kubectl get <kind> -l app=<name>` queries for discovering all resources belonging to a project, which aids operational debugging and bulk cleanup.
- reqwest 0.13 renamed the `rustls-tls` feature to `rustls`. The `.query()` and `.form()` RequestBuilder methods are now behind opt-in feature flags (`query`, `form`). TLS-related ClientBuilder methods got `tls_` prefixes (old names still work but are deprecated). If using `default-features = false`, the only required change is the feature rename.
- K8s pods with sidecar dependencies need startup probes, not just liveness/readiness probes. Without a startup probe, the liveness probe's `initialDelaySeconds` is a fixed guess at how long startup takes. A startup probe with `failureThreshold * periodSeconds` provides a proper startup budget (e.g. 30 * 2s = 60s) and once it succeeds, liveness/readiness probes take over. This prevents premature restarts when the sidecar (tailscaled) takes longer than expected to authenticate.

- RUNBOOK example responses must reflect actual runtime behavior, not idealized static values. The degraded health endpoint returns real `uptime_seconds`, `requests_served`, and `errors_total` values (not zeros), because these counters run regardless of tailnet connection state. Hardcoding zeros in documentation misleads operators into thinking these fields are meaningless when degraded.
- `docker/build-push-action@v6` with `cache-from/cache-to: type=gha` requires `docker/setup-buildx-action@v3` in the workflow. The default docker driver does not support GHA cache export. Without the buildx setup step, the build fails with "Cache export is not supported for the docker driver."
- `debian:bookworm-slim` includes a system user named `proxy` (UID 13). Creating a custom user with `useradd ... proxy` fails. Use a different username like `appuser` for application-specific non-root users to avoid conflicts with Debian standard system users.
- Tailscale container image v1.94.1 has K8s-aware startup that tries to manage its own state via K8s secrets. If running as a sidecar without RBAC for secret access, set `TS_KUBE_SECRET=""` to disable K8s secret storage and fall back to filesystem state in the `TS_STATE_DIR` emptyDir volume. Without this, the sidecar fails with "missing get permission on secret tailscale".
- Private GitHub repos produce private GHCR packages. The `GITHUB_TOKEN` in GitHub Actions has `packages:write` for push/pull registry operations but fundamentally cannot change package visibility via the REST API — this is a long-standing GitHub limitation (since 2021, still unresolved). A CI step using `gh api --method PATCH /user/packages/... -f visibility=public` with `GITHUB_TOKEN` silently fails even with `|| true`. The only ways to change visibility are: (1) the GitHub web UI, (2) a classic PAT with `write:packages` scope, or (3) changing account-level default package visibility to "Public" in Settings → Packages. The `gh` CLI `gho_` OAuth token has `repo` scope but not `read:packages`, so even `gh api` calls to query packages fail with 403.
- Tailscale auth keys for sidecar pods should be **reusable** and **ephemeral**. Single-use keys get consumed on the first pod creation and fail on restarts. Ephemeral keys auto-deregister the node when it goes offline, preventing stale device accumulation. Create via Tailscale API using the operator's OAuth credentials: exchange OAuth client credentials for an access token, then POST to `/api/v2/tailnet/-/keys`.
- Kubernetes `capabilities.add` is inert for non-root containers on containerd. Containerd clears the ambient capability set for non-root users, so added capabilities sit in bounding/inheritable sets but never reach the effective set where syscalls check them. `allowPrivilegeEscalation: false` compounds this via `no_new_privs`, but even without it, ambient caps are cleared. This is containerd issue #5644 (closed "not planned") and KEP-2763 (unimplemented). The 37th audit's `CAP_FOWNER` fix was a no-op.
- Tailscale v1.94.1 `ensureStateDirPermsUnix` calls `chmod` on the state directory when permissions don't match `0700`. With `fsGroup: 1000`, Kubernetes sets directory permissions to `2755` (setgid), triggering the chmod. The function has a guard: it only runs when `filepath.Base(dir) == "tailscale"`. Fix: rename the state directory from `/var/lib/tailscale` to `/var/lib/ts-state` so the basename bypasses the guard entirely. This avoids needing capabilities, init containers, or running as root.
- K8s deployment requires both GHCR package access AND Tailscale auth key secret. The ghcr-pull-secret exists but its token lacks `read:packages` scope. Both must be functional before pods can start.
- K8s deployments using mutable image tags (`:main`, `:latest`) must set `imagePullPolicy: Always` explicitly. The Kubernetes default `IfNotPresent` caches images by tag, so updated images pushed to the same tag are never pulled. Pinned tags (`:v1.94.1`) can use `IfNotPresent` since the content is immutable.
- CI workflows with `docker/metadata-action` semver tag patterns (`type=semver,pattern={{version}}`) require the workflow `on.push.tags` trigger to include `v*` or equivalent. Without it, tag pushes never fire the workflow and the semver metadata patterns are dead code. The push condition also needs updating to allow tag refs, not just `refs/heads/main`.
- Prometheus pod annotations (`prometheus.io/scrape`, `prometheus.io/port`, `prometheus.io/path`) must be on the pod template metadata, not the Deployment metadata. Prometheus discovers scrape targets at the pod level. Without these, the RUNBOOK's "scrape `/metrics` on port 8080" instruction has no mechanism for auto-discovery.
- Docker builds without a `.dockerignore` send the entire repository as build context to the daemon, including `.git/`, `.specstory/`, `k8s/`, `specs/`, and documentation. None of these are needed in the image. A `.dockerignore` reduces context transfer time and prevents accidental inclusion of sensitive files.
- K8s `imagePullSecrets` that reference nonexistent secrets are tolerated when anonymous pulls succeed (public registry). But if the image is private, the missing secret causes `ErrImagePull` with no hint about the secret — operators need clear documentation of all prerequisite secrets.

## Environment Notes

- Rust toolchain: cargo 1.93.0, rustc 1.93.0 (stable, Jan 2026), installed at `~/.cargo/bin/cargo`
- Cross-compilation: `cargo-zigbuild` v0.21.6 + zig 0.15.2 for Linux targets
