# Implementation Plan

Phases 1-5 complete. All 106 tests pass (97 oauth-proxy + 9 common). Binary sizes well under 15MB target. Specs updated with resolved decisions.

Audits 1-48: Found and fixed 70+ issues across 48 audits including 5 bugs, spec documentation gaps, K8s security context issues, state machine correctness, metrics configuration, RUNBOOK accuracy, dependency upgrades, CI/CD blockers, live cluster deployment fixes, and test coverage gaps. Key milestones: 34th audit (v0.0.48) first clean audit; 44th audit (v0.0.63) deployment went live; 46th-49th audits found only test coverage gaps and dependency updates as the codebase stabilized.

Fiftieth audit (v0.0.70): Deep cross-cutting audit across spec, RUNBOOK, K8s manifests, CI, and Dockerfile. Verified full spec alignment section-by-section (all types, states, events, actions, transitions, HTTP behaviors, error codes, metrics, config options, dependency versions, and release profile match). Found 3 actionable improvements: (1) K8s deployment missing explicit `TAILSCALE_SOCKET` env var on proxy container — defaults matched but created a fragile implicit contract with the tailscaled sidecar's `TS_SOCKET` setting; now explicit, (2) liveness probe had no `failureThreshold`, making it overly aggressive during transient tailnet disconnects — a single missed probe would restart the pod; now set to 3 (tolerates 45s of tailnet unavailability before restart), (3) CI had no dependency vulnerability scanning — added `cargo audit` job using the RustSec Advisory Database, gating Docker builds on clean audit results. RUNBOOK fully accurate. All 104 tests pass, clippy clean, fmt clean.

Fifty-first audit (v0.0.71): Full spec-vs-implementation audit verified 99.5% compliance across all spec sections. Found 1 actionable issue: RUNBOOK PromQL alert query `histogram_quantile(0.99, rate(proxy_request_duration_seconds_bucket[5m]))` was missing `sum by (le)` aggregation — the histogram carries a `status` label, so without aggregation `histogram_quantile` receives multiple series per `le` bucket and produces incorrect results or errors. Fixed to `histogram_quantile(0.99, sum by (le) (rate(...)))`. All 104 tests pass, clippy clean, fmt clean.

Fifty-second audit (v0.0.72): Comprehensive audit with parallel subagents across all source files, K8s manifests, CI, Dockerfile, RUNBOOK, and both specs. Verified 100% spec compliance across proxy.rs, main.rs, service.rs, config.rs, metrics.rs, error.rs, tailnet.rs. RUNBOOK fully accurate (PromQL, env vars, troubleshooting, secret rotation). K8s manifests correctly configured (pod security context, probes, resource limits, Prometheus annotations). CI workflow correct (all 5 jobs, proper dependency chain, semver triggers, binary size check). Dockerfile meets all requirements. Deep code audit found no dangerous unwrap() calls, no silent error swallowing, no TODO/FIXME comments, and all dependencies current. Found 2 test coverage gaps and added tests: (1) config deserialization error on valid TOML with missing required fields (missing [proxy] section, missing listen_addr, missing hostname), (2) proxy body re-send on timeout retry (verifies body_bytes.clone() correctly delivers original body on each retry attempt). All 106 tests pass, clippy clean, fmt clean.

Fifty-third audit (v0.0.73): Full audit with parallel subagents across source code, K8s manifests, RUNBOOK, CI, and Dockerfile. Code is 100% spec-compliant; no source changes needed. Found 3 K8s/doc improvements: (1) readinessProbe missing explicit `failureThreshold` — Kubernetes defaults to 3 but the 50th audit learning says to document intent explicitly; now set to 3, (2) tailscaled sidecar had no liveness probe — if tailscaled crashes and the Unix socket disappears, Kubernetes had no way to detect or restart it; added exec probe checking socket existence (`test -S /var/run/tailscale/tailscaled.sock`) with 30s period and 3 failure threshold (90s tolerance), (3) no README.md — project root had no documentation for new contributors; added with project overview, quick-start, structure, and pointers to specs/RUNBOOK. RUNBOOK updated with tailscaled liveness probe documentation. All 106 tests pass, clippy clean, fmt clean.

Fifty-fourth audit (v0.0.74): Full audit across all source files, K8s manifests, RUNBOOK, CI, Dockerfile, and both specs. Code is 100% spec-compliant; all dependencies current; no source changes needed. Found 1 RUNBOOK documentation gap: the startup probe (30 failures × 2s = 60-second budget for initial tailnet connection) was referenced in the Endpoints table but not explained in the Troubleshooting section — operators seeing CrashLoopBackOff during slow tailnet connections had no documentation explaining the 60-second startup window or that liveness/readiness probes are suppressed during it. Added startup probe explanation to the "Pod CrashLoopBackOff" troubleshooting section. Also verified: RUNBOOK tailscaled liveness tolerance math is correct (3 × 30s = 90s), tailscaled does not need a readiness probe (proxy's own startup probe already gates pod readiness on tailscaled availability), all probe configurations match documented values. All 106 tests pass, clippy clean, fmt clean.

Fifty-fifth audit (v0.0.74): Comprehensive deep audit across every source file, K8s manifest, CI workflow, Dockerfile, RUNBOOK, README, and both specs. Zero critical, major, or minor issues found. Code is 100% spec-compliant across all modules (proxy.rs, main.rs, service.rs, config.rs, metrics.rs, error.rs, tailnet.rs). All 106 tests pass, clippy clean, fmt clean. All dependencies current — one transitive dependency (matchit 0.8.4 → 0.8.6) has a newer version available but cannot be updated because axum v0.8.8 pins `matchit = "=0.8.4"` exactly. No unwrap() calls in production code paths (all 4 are initialization-time expects that fail-fast before the server starts). No race conditions, no logic bugs, no off-by-one errors, no TODO/FIXME comments. K8s manifests fully correct (probes, security contexts, resource limits, labels, annotations). CI workflow complete and secure (least-privilege permissions). Dockerfile follows all best practices. RUNBOOK fully accurate (PromQL, env vars, troubleshooting, resource limits table). README accurate and minimal. Second consecutive clean audit with no code changes needed.

## Remaining Work

All implementation items complete. Aperture integration verified with live E2E traffic on 2026-02-06:

- [x] Aperture config updated with `anthropic-oauth` provider, `tailnet: true` routing
- [x] ACL connectivity from Aperture verified (Metric ID: 235 in Aperture dashboard)
- [x] Production traffic flowing — Claude API responses confirmed with header injection
- [ ] Long-term production monitoring — observe traffic patterns, error rates, and resource usage over time
- [ ] Load testing — verify 100+ req/s sustained (spec success criterion)
- [ ] Memory soak testing — verify zero memory growth over 24h (spec success criterion)

## Known Limitations

- Health endpoint `tailnet` state is set once at startup and never updated during operation. If tailscaled drops during runtime, health still reports `"connected"`. Fixing this requires tailnet health monitoring (periodic polling of tailscaled), which is infrastructure work beyond the current spec. The `tailnet_connected` Prometheus gauge does get set to `false` during graceful shutdown.
- `ConfigError` and `ListenerBindError` are not in the service's Rust error enum. Config errors use `common::Error` and listener bind errors use `anyhow`. These paths work correctly; the spec now documents this split explicitly.

## Learnings

- Reverse proxies must strip the client's `host` header before forwarding. The client sends `Host: <proxy-address>` but the upstream expects `Host: <upstream-address>`. HTTP client libraries like reqwest automatically set the correct Host from the URL, but only if the incoming Host isn't manually set in the header map.
- Config-driven header injection must protect safety-critical headers. The `authorization` header should never be overwritable via config, even if someone misconfigures it. This is a system boundary validation.
- When copying HTTP headers in a proxy, use `append()` not `insert()` to preserve multi-value headers. `insert()` replaces, `append()` accumulates. This matters for headers like Cookie, Accept-Encoding, and custom multi-value headers.
- Rust 2024 edition requires `unsafe {}` blocks inside `unsafe fn` bodies. Tests that call `std::env::set_var`/`remove_var` (unsafe since Rust 1.83) need both the `unsafe fn` wrapper and inner `unsafe {}` blocks.
- Tests that mutate environment variables must be serialized with a `Mutex` to prevent data races when `cargo test` runs in parallel (default behavior). Without this, env-var-dependent tests fail nondeterministically.
- `tracing-subscriber` requires the `env-filter` feature for `EnvFilter` support. The `json` feature alone is not sufficient.
- Drain coordination: axum's `with_graceful_shutdown` handles connection-level draining (stops accepting new connections, waits for in-flight to finish), but it waits indefinitely by default. The spec requires a 5-second drain timeout. Enforced by spawning the server as a task, signaling it via a `oneshot` channel on SIGTERM/SIGINT, then racing the drain against `DRAIN_TIMEOUT` using `tokio::time::timeout`.
- `tailscale-localapi` v0.4.2 uses `chrono` for timestamps and brings in `hyper` v0.14 (in addition to the workspace's `hyper` v1). This is expected — the crate was built against an older `hyper` API.
- On macOS with the App Store Tailscale variant, there is no Unix socket. The fallback is `tailscale status --json` which parses the same `Status` type via `serde_json`.
- `metrics-exporter-prometheus` global recorder can only be installed once per process. In tests, use `PrometheusBuilder::build_recorder()` + `.handle()` to create isolated instances without global installation.
- Integration tests using `tower::ServiceExt::oneshot` give full end-to-end coverage without needing to bind a TCP port — they call the axum router directly as a tower Service.
- Cross-compilation from macOS to Linux requires `cargo-zigbuild` (uses zig as the C cross-linker). Standard `cargo build --target` fails because `aws-lc-sys` needs a C cross-compiler.
- `reqwest` with default features enabled pulls in `native-tls` → `openssl-sys` on Linux targets, even when `rustls-tls` is also enabled. Setting `default-features = false` is required to avoid the OpenSSL dependency.
- `tower` crate requires explicit feature flags for each layer type. `ConcurrencyLimitLayer` requires the `limit` feature.
- `BackendState` enum in `tailscale-localapi` is `#[non_exhaustive]`, requiring wildcard match arms.
- Tower's `ConcurrencyLimitLayer` queues excess requests rather than rejecting them. Requests above `max_connections` will wait (not fail) until a slot opens.
- Docker build uses native `x86_64-unknown-linux-gnu` target (not musl) inside `rust:1-bookworm`. No cross-compilation needed since Docker IS Linux.
- K8s manifests use `TS_USERSPACE=true` for the tailscaled sidecar to avoid requiring `NET_ADMIN` capabilities. The proxy and tailscaled share the Unix socket via an `emptyDir` volume.
- GitHub Actions CI uses `actions/checkout@v6`, `dtolnay/rust-toolchain@stable`, and `Swatinem/rust-cache@v2`. All jobs have explicit least-privilege `permissions` blocks. Docker job uses `docker/build-push-action@v6` with GHA cache. Images push to GHCR using the built-in `GITHUB_TOKEN`.
- `BackendState::NeedsMachineAuth` requires manual admin approval in the Tailscale console. Mapping it to a retryable error wastes 31 seconds of exponential backoff before giving up. It must be non-retryable.
- A spec-vs-implementation audit is valuable after completing major phases. Found 43+ discrepancies across ten audits including 5 bugs, spec documentation gaps, and positive deviations. The tenth audit found 1 state machine bug (terminal state not fully inert).
- Terminal states in a state machine must be explicitly guarded before wildcard match arms. Without a `Stopped` guard before `(_, ShutdownSignal)`, the wildcard produces a `Shutdown` action from an already-stopped state, violating the "terminal means inert" invariant.
- K8s sidecar pattern requires both containers to mount the shared volume. The volume definition in `spec.volumes` is not enough — each container that needs the socket must have a `volumeMount` entry. Easy to miss because the tailscaled container (which creates the socket) works fine; only the consumer (proxy) fails.
- Response bodies must be streamed, not buffered, in a proxy targeting the Claude API. The Anthropic API uses SSE (Server-Sent Events) for streaming responses. Buffering breaks real-time delivery and uses unbounded memory. Use `reqwest::Response::bytes_stream()` with `axum::body::Body::from_stream()`. Metrics (status, duration) must be collected before consuming the stream since headers are available immediately.
- Config validation at system boundaries catches misconfigurations early: `upstream_url` must have an http(s) scheme, `timeout_secs` and `max_connections` must be non-zero. Without URL scheme validation, reqwest fails at request time with a confusing error instead of at startup.
- `metrics-exporter-prometheus` renders `metrics::histogram!()` as a Prometheus summary (quantiles) by default. To get a true histogram (with `_bucket` lines needed by `histogram_quantile()` queries), you must configure explicit bucket boundaries via `set_buckets_for_metric()`. Without this, RUNBOOK PromQL queries referencing `_bucket` will fail silently.
- In a sidecar pattern, secrets should only be mounted in the container that consumes them. `TS_AUTHKEY` belongs on the tailscaled sidecar, not the proxy container — the proxy queries tailnet state via the Unix socket and never authenticates directly.
- Spec dependency lists can drift from the actual Cargo.toml when features are added during implementation. The `"stream"` feature on reqwest was added for response streaming but the spec's Build & Distribution section was not updated. Always update the spec when adding dependency features.
- Dockerfiles for K8s pods with `runAsNonRoot: true` must create the non-root user in the image. `debian:bookworm-slim` only has root; use `useradd -u 1000 -r -s /sbin/nologin appuser` and `USER 1000` in the runtime stage. Without this, the pod crashes with `CreateContainerConfigError`. Avoid the username `proxy` — it's a Debian standard system user (UID 13).
- `reqwest::Client::new()` uses unbounded connection pool defaults. For a proxy with configurable `max_connections`, set `connect_timeout()` and `pool_max_idle_per_host()` on the builder to prevent unbounded TCP connections when upstream is slow to accept.
- K8s Pod Security Standards (restricted profile) require `allowPrivilegeEscalation: false`, `readOnlyRootFilesystem: true`, and `capabilities: { drop: ["ALL"] }` on every container. Missing these can block deployment to hardened clusters.
- Using `:latest` for sidecar images in K8s deployments breaks reproducibility and rollbacks. Pin to specific versions (e.g. `tailscale:v1.94.1`) so that `kubectl rollout undo` works predictably.
- State machine variants should only carry data they own and use. The `Running` state had a `ServiceMetrics` that was never read because `main.rs` creates its own metrics instance wired to `ProxyState`. Dead allocations in state variants waste memory and confuse readers.
- K8s `terminationGracePeriodSeconds` should be DRAIN_TIMEOUT + small buffer (e.g. 1s), not significantly larger. The application force-exits after DRAIN_TIMEOUT regardless, so the extra Kubernetes wait is wasted delay during rolling updates and node drains.
- Kustomize secrets with placeholder values overwrite real secrets on `kubectl apply -k`. If a secret contains a real credential created imperatively, do NOT include it in `kustomization.yaml`. Keep a schema-documenting `secret.yaml` in the repo but excluded from kustomization resources. The RUNBOOK should instruct users to create the secret imperatively after `kubectl apply -k`.
- K8s Pod Security Standards restricted profile requires `runAsNonRoot: true` on every container, not just the main application container. Setting `runAsUser: 1000` is not sufficient — the explicit `runAsNonRoot` field is what Kubernetes admission controllers check. Missing it on sidecar containers is easy to overlook.
- Prometheus histograms and summaries are different metric types with different semantics. Histograms produce `_bucket`, `_sum`, and `_count` lines; quantiles are computed at query time via `histogram_quantile()`. Summaries compute quantiles client-side. Documentation must use precise terminology — saying a histogram "automatically computes quantiles" is misleading and confuses operators writing PromQL.
- Undocumented environment variable overrides create debugging blind spots. If code reads an env var to override defaults (like `TAILSCALE_SOCKET` for the socket path), it must be documented in both the spec's environment variables table and the operational runbook's troubleshooting section.
- Crate `derive` features (e.g. `zeroize = { features = ["derive"] }`) pull in proc-macro dependencies (`syn`, `quote`, `proc-macro2`). Only enable them if `#[derive(Trait)]` is actually used. Using a trait as a bound or calling methods directly does not require the derive feature.
- Concurrency limits on a proxy must exclude observability endpoints. K8s liveness/readiness probes and Prometheus scrapes must always be responsive regardless of proxy load. In axum, use `Router::merge()` to nest a concurrency-limited sub-router (proxy routes) under an unlimited parent router (health/metrics routes).
- K8s secret rotation should use `kubectl create --dry-run=client -o yaml | kubectl apply -f -` for atomic updates. A `delete` then `create` sequence leaves a window where pods rescheduled between the two commands fail with `CreateContainerConfigError`.
- Minimal Docker images (debian-slim + ca-certificates only) don't have debugging tools like `curl`. RUNBOOK troubleshooting steps should use `kubectl port-forward` from the operator's workstation instead of `kubectl exec` with tools that aren't in the image.
- K8s restricted pod security profile requires pod-level `securityContext` with `seccompProfile.type: RuntimeDefault`. Container-level security contexts alone are insufficient — admission controllers check the pod-level seccomp profile separately. Also set `fsGroup` at the pod level so emptyDir volumes are writable by the non-root group.
- `unreachable!()` in non-test code paths is a latent process abort, especially with `panic = "abort"` in the release profile. Even if the current caller never triggers the arm, future code changes might. Replace `unreachable!()` with defensive no-op returns in state machines where the arm is theoretically reachable but practically unused.
- K8s resources should carry consistent `app:` labels even if they are not selected by anything. Labels enable `kubectl get <kind> -l app=<name>` queries for discovering all resources belonging to a project, which aids operational debugging and bulk cleanup.
- reqwest 0.13 renamed the `rustls-tls` feature to `rustls`. The `.query()` and `.form()` RequestBuilder methods are now behind opt-in feature flags (`query`, `form`). TLS-related ClientBuilder methods got `tls_` prefixes (old names still work but are deprecated). If using `default-features = false`, the only required change is the feature rename.
- K8s pods with sidecar dependencies need startup probes, not just liveness/readiness probes. Without a startup probe, the liveness probe's `initialDelaySeconds` is a fixed guess at how long startup takes. A startup probe with `failureThreshold * periodSeconds` provides a proper startup budget (e.g. 30 * 2s = 60s) and once it succeeds, liveness/readiness probes take over. This prevents premature restarts when the sidecar (tailscaled) takes longer than expected to authenticate.

- RUNBOOK example responses must reflect actual runtime behavior, not idealized static values. The degraded health endpoint returns real `uptime_seconds`, `requests_served`, and `errors_total` values (not zeros), because these counters run regardless of tailnet connection state. Hardcoding zeros in documentation misleads operators into thinking these fields are meaningless when degraded.
- `docker/build-push-action@v6` with `cache-from/cache-to: type=gha` requires `docker/setup-buildx-action@v3` in the workflow. The default docker driver does not support GHA cache export. Without the buildx setup step, the build fails with "Cache export is not supported for the docker driver."
- `debian:bookworm-slim` includes a system user named `proxy` (UID 13). Creating a custom user with `useradd ... proxy` fails. Use a different username like `appuser` for application-specific non-root users to avoid conflicts with Debian standard system users.
- Tailscale container image v1.94.1 has K8s-aware startup that tries to manage its own state via K8s secrets. If running as a sidecar without RBAC for secret access, set `TS_KUBE_SECRET=""` to disable K8s secret storage and fall back to filesystem state in the `TS_STATE_DIR` emptyDir volume. Without this, the sidecar fails with "missing get permission on secret tailscale".
- Private GitHub repos produce private GHCR packages. The `GITHUB_TOKEN` in GitHub Actions has `packages:write` for push/pull registry operations but fundamentally cannot change package visibility via the REST API — this is a long-standing GitHub limitation (since 2021, still unresolved). A CI step using `gh api --method PATCH /user/packages/... -f visibility=public` with `GITHUB_TOKEN` silently fails even with `|| true`. The only ways to change visibility are: (1) the GitHub web UI, (2) a classic PAT with `write:packages` scope, or (3) changing account-level default package visibility to "Public" in Settings → Packages. The `gh` CLI `gho_` OAuth token has `repo` scope but not `read:packages`, so even `gh api` calls to query packages fail with 403.
- Tailscale auth keys for sidecar pods should be **reusable** and **ephemeral**. Single-use keys get consumed on the first pod creation and fail on restarts. Ephemeral keys auto-deregister the node when it goes offline, preventing stale device accumulation. Create via Tailscale API using the operator's OAuth credentials: exchange OAuth client credentials for an access token, then POST to `/api/v2/tailnet/-/keys`.
- Kubernetes `capabilities.add` is inert for non-root containers on containerd. Containerd clears the ambient capability set for non-root users, so added capabilities sit in bounding/inheritable sets but never reach the effective set where syscalls check them. `allowPrivilegeEscalation: false` compounds this via `no_new_privs`, but even without it, ambient caps are cleared. This is containerd issue #5644 (closed "not planned") and KEP-2763 (unimplemented). The 37th audit's `CAP_FOWNER` fix was a no-op.
- Tailscale v1.94.1 `ensureStateDirPermsUnix` calls `chmod` on the state directory when permissions don't match `0700`. With `fsGroup: 1000`, Kubernetes sets directory permissions to `2755` (setgid), triggering the chmod. The function has a guard: it only runs when `filepath.Base(dir) == "tailscale"`. Fix: rename the state directory from `/var/lib/tailscale` to `/var/lib/ts-state` so the basename bypasses the guard entirely. This avoids needing capabilities, init containers, or running as root.
- K8s deployment requires both GHCR package access AND Tailscale auth key secret. The ghcr-pull-secret exists but its token lacks `read:packages` scope. Both must be functional before pods can start.
- K8s deployments using mutable image tags (`:main`, `:latest`) must set `imagePullPolicy: Always` explicitly. The Kubernetes default `IfNotPresent` caches images by tag, so updated images pushed to the same tag are never pulled. Pinned tags (`:v1.94.1`) can use `IfNotPresent` since the content is immutable.
- CI workflows with `docker/metadata-action` semver tag patterns (`type=semver,pattern={{version}}`) require the workflow `on.push.tags` trigger to include `v*` or equivalent. Without it, tag pushes never fire the workflow and the semver metadata patterns are dead code. The push condition also needs updating to allow tag refs, not just `refs/heads/main`.
- Prometheus pod annotations (`prometheus.io/scrape`, `prometheus.io/port`, `prometheus.io/path`) must be on the pod template metadata, not the Deployment metadata. Prometheus discovers scrape targets at the pod level. Without these, the RUNBOOK's "scrape `/metrics` on port 8080" instruction has no mechanism for auto-discovery.
- Docker builds without a `.dockerignore` send the entire repository as build context to the daemon, including `.git/`, `.specstory/`, `k8s/`, `specs/`, and documentation. None of these are needed in the image. A `.dockerignore` reduces context transfer time and prevents accidental inclusion of sensitive files.
- K8s `imagePullSecrets` that reference nonexistent secrets are tolerated when anonymous pulls succeed (public registry). But if the image is private, the missing secret causes `ErrImagePull` with no hint about the secret — operators need clear documentation of all prerequisite secrets.
- K8s liveness probes without `failureThreshold` default to 3 failures, but explicitly setting it documents intent and prevents confusion. For services with sidecar dependencies (tailscaled), the liveness probe must tolerate transient disconnects — if the tailnet coordinator is briefly unreachable, restarting the pod only makes things worse. Set `failureThreshold` to match the expected reconnection window (e.g. 3 failures × 15s period = 45s tolerance).
- Environment variables that create implicit contracts between containers in a pod should be explicit on both sides. If the tailscaled sidecar sets `TS_SOCKET=/var/run/tailscale/tailscaled.sock` and the proxy defaults to the same path, the contract is fragile — changing one side silently breaks the other. Setting `TAILSCALE_SOCKET` explicitly on the proxy container documents the dependency and makes mismatches immediately visible in `kubectl describe`.
- `cargo audit` scans Cargo.lock against the RustSec Advisory Database for known vulnerability advisories. Unlike `cargo clippy` (which finds code patterns), `cargo audit` catches supply-chain issues in dependencies that the project has no control over. Adding it as a CI gate catches advisories before they reach production images.
- PromQL `histogram_quantile()` requires a single time series per `le` bucket. When a histogram has additional labels (e.g. `status`), the query must use `sum by (le)` to aggregate across label values before passing to `histogram_quantile()`. Without this, the function receives multiple series per bucket and produces incorrect results or "not a valid histogram" errors. Easy to miss in runbooks because the query works fine with a single status code but breaks as soon as a second status appears.
- Tailscaled sidecar containers need liveness probes even though they don't expose HTTP health endpoints. The Unix socket at the shared `emptyDir` volume serves as a health signal — if tailscaled crashes, the socket file disappears. Use `exec` probe with `test -S <socket-path>` to detect this. Set `initialDelaySeconds` to give tailscaled time to create the socket on startup, and use a generous `failureThreshold × periodSeconds` window (e.g. 90s) to tolerate transient issues without unnecessary restarts.
- K8s probe fields that default to reasonable values (like `failureThreshold: 3`) should still be set explicitly when the value represents a deliberate operational decision. Implicit defaults work but don't communicate intent — future operators may not know whether the omission was deliberate or accidental.
- Transitive dependency updates may be blocked by exact version pins in upstream crates. axum v0.8.8 pins `matchit = "=0.8.4"` (exact), so `cargo update matchit --precise 0.8.6` fails. The only fix is waiting for axum to release with the updated pin. `cargo update --dry-run --verbose` reveals these blocked updates.

## Environment Notes

- Rust toolchain: cargo 1.93.0, rustc 1.93.0 (stable, Jan 2026), installed at `~/.cargo/bin/cargo`
- Cross-compilation: `cargo-zigbuild` v0.21.6 + zig 0.15.2 for Linux targets
